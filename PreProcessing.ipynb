{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from mne.preprocessing import ICA\n",
    "import pandas as pd\n",
    "from mne.minimum_norm import apply_inverse, make_inverse_operator\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "In this tutorial, you will learn how to analyze data from an MEG study involving two blocks. The workflow is applicable to studies with one block, or 3+, as well. Basically, you need to perform preprocessing separately on every continuous block of data that you collect, and the way that you preprocess each block should be kept consistent to allow for comparisons across blocks.\n",
    "\n",
    "In this tutorial, one block is a primed word-reading task (comp), and the other is a primed picture-naming task (prod). This means that there are two experimental files, which need to be preprocessed equivalently so that they may be compared to one another.\n",
    "\n",
    "![trial](Trial.jpg \"Trial\")"
   ],
   "id": "4026e9bd2a319ae6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Custom functions\n",
    "\n",
    "Specific to the UMD MEG system, it is necessary to manually correct the location of (0-indexed) MEG sensor 56. The .fif file type has an attribute which identifies the locations of every channel, indexed via *raw.info['chs'][idx]['loc']*, where idx is the index of a channel whose location you want to view. It's important that the location of all channels is correct for many parts of the preprocessing pipeline, like interpolating bad channels.\n",
    "\n",
    "We have the location value that MEG 056 *should* have. The function below will correct the sensor location and should be called for all files recorded using the UMD MEG system."
   ],
   "id": "e847a6522d304cf0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def fix_56(raw):\n",
    "    # Fix the location of MEG 056, necessary for all data collected at KIT-UMD MEG Lab\n",
    "    loc = np.array([\n",
    "        0.09603, -0.07437, 0.00905, -0.5447052, -0.83848277,\n",
    "        0.01558496, 0., -0.01858388, -0.9998273, 0.8386276,\n",
    "        -0.54461113, 0.01012274])\n",
    "    index = raw.ch_names.index('MEG 056')\n",
    "    raw.info['chs'][index]['loc'] = loc\n",
    "    return raw"
   ],
   "id": "e16c58502ad4f7b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Importing Files\n",
    "\n",
    "Change the ROOT filepath to the directory that the tutorial is stored in on your computer. Everything else *should* be fine to run without editing, save for the ICA components to remove."
   ],
   "id": "a4a9f35b094701ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sub = 'R3289'\n",
    "condition = 'G'"
   ],
   "id": "214040e9c8bf0b53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ROOT = f\"/Users/audreylaun/Library/CloudStorage/Box-Box/UMD_MEG_Admin_Only/Analysis/\"\n",
    "input_dir = f\"{ROOT}Tutorial/{sub}/\"\n",
    "output_dir = f\"{ROOT}Tutorial/{sub}/Output\""
   ],
   "id": "842f33c93ad0c57e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For this experiment, it's necessary to store the conditions for the comprehension and production blocks, these can be obtained based on the overall condition value (A-H)",
   "id": "9e147e28ef11f01d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "comp_condition = 'X'\n",
    "prod_condition = 'X'\n",
    "if condition in ['A', 'D']:\n",
    "    comp_condition = 'A1'\n",
    "    prod_condition = 'B1'\n",
    "elif condition in ['B','C']:\n",
    "    comp_condition = 'B1'\n",
    "    prod_condition = 'A1'\n",
    "elif condition in ['E','H']:\n",
    "    comp_condition = 'A2'\n",
    "    prod_condition = 'B2'\n",
    "elif condition in ['F', 'G']:\n",
    "    comp_condition = 'B2'\n",
    "    prod_condition = 'A2'"
   ],
   "id": "c1a92b39b85f972c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We will load experimental files (comp-raw and prod-raw) *and* empty room files (emptyroom). The empty room file can be used for denoising, or noise covariance. In this specific tutorial, we do not use the empty room. Nevertheless, it is still good practice to preprocess the empty room file the exact same way that you preprocess your experimental data, in case it becomes useful down the line.\n",
    "\n",
    "The experimental files loaded here have already been converted from .sqd to .fif using the kit2fiff terminal GUI.\n",
    "\n",
    "The empty room is still a sqd file. The conversion is not necessary because we do not need to set the head shape/marker measurement attributes because there are none for these data. Instead, we will assign the info from an experimental fif file so that we can Maxwell filter the empty room file the same way as the experimental blocks."
   ],
   "id": "a072ea83464216c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "comp_raw_fname = input_dir + sub + '_comp-raw.fif'\n",
    "prod_raw_fname = input_dir + sub + '_prod-raw.fif'\n",
    "empty_room_fname = input_dir + '/DAQ/' + sub + '_emptyroom.sqd'\n",
    "\n",
    "comp_raw = mne.io.read_raw_fif(comp_raw_fname, preload=True)\n",
    "prod_raw = mne.io.read_raw_fif(prod_raw_fname, preload=True)\n",
    "empty_room_raw = mne.io.read_raw_kit(empty_room_fname, preload=True)\n",
    "empty_room_raw = mne.io.RawArray(empty_room_raw.get_data(), comp_raw.info)"
   ],
   "id": "4c3cda9ef0e48ace",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fix the location of sensor 56\n",
    "\n",
    "We call our custom function is called to fix the location for all files."
   ],
   "id": "7b4cbb1c459a6e74"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "comp_raw = fix_56(comp_raw)\n",
    "prod_raw = fix_56(prod_raw)\n",
    "empty_room_raw = fix_56(empty_room_raw)"
   ],
   "id": "a2fa2cf99c27c25d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Excluding bad channels",
   "id": "845e559118e44fe7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "MEG 056 and MEG 086 are always bad. 056 shows up inside the head and, even though the position can be corrected, the measurements it takes are not trustworthy. 086 is flat. We will always mark these as bad",
   "id": "e2f6dd7bd56b1ffd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bads = ['MEG 056', 'MEG 086']",
   "id": "50f60a2ce376d621",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Other sensors are *sometimes* bad. We will call the [mne.preprocessing.find_bad_channels_maxwell_](https://mne.tools/stable/generated/mne.preprocessing.find_bad_channels_maxwell.html) function to automatically detect these channels\n",
    "\n",
    "We will look for bad channels in both experimental files, creating a \"master list\" of bads, which will be excluded from both experimental files and the empty room."
   ],
   "id": "7a64763200e018b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "noisy_chs, flat_chs = mne.preprocessing.find_bad_channels_maxwell(\n",
    "        comp_raw, ignore_ref=True\n",
    "    )\n",
    "for i in bads:\n",
    "    if i not in noisy_chs:\n",
    "        noisy_chs.append(i)\n",
    "noisy_chs, flat_chs = mne.preprocessing.find_bad_channels_maxwell(\n",
    "        prod_raw, ignore_ref=True\n",
    "    )\n",
    "for i in bads:\n",
    "    if i not in noisy_chs:\n",
    "        noisy_chs.append(i)\n",
    "\n",
    "\n",
    "comp_raw.info['bads'] = noisy_chs\n",
    "prod_raw.info['bads'] = noisy_chs\n",
    "empty_room_raw.info['bads'] = noisy_chs"
   ],
   "id": "73f28ef6cf06f980",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Maxwell Filtering",
   "id": "8f5bce4e5d5285d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We will use the [mne.preprocessing.maxwell_filter](https://mne.tools/stable/generated/mne.preprocessing.maxwell_filter.html#mne.preprocessing.maxwell_filter) function to filter the data using mutipole moments. Essentially, this preprocessing step will clean the data by suppressing signals from distant sources.\n",
    "\n",
    "Because of our specific MEG system, it is important to specifically set some parameters for this function. Specifically, we need to set **ignore_ref** to **True** and **st_only** to **True**. This means that we will only perform temporal projections (tSSS instead of SSS) on the output data because our the KIT system does not allow for cross-talk cancellation and movement compensation."
   ],
   "id": "fac6f1a39624adb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "comp_raw_tsss = mne.preprocessing.maxwell_filter(\n",
    "        comp_raw,\n",
    "        st_duration=10,\n",
    "        ignore_ref=True,\n",
    "        st_correlation=0.9,\n",
    "        st_only=True\n",
    "    )\n",
    "\n",
    "prod_raw_tsss = mne.preprocessing.maxwell_filter(\n",
    "    prod_raw,\n",
    "    st_duration=10,\n",
    "    ignore_ref=True,\n",
    "    st_correlation=0.9,\n",
    "    st_only=True\n",
    ")\n",
    "\n",
    "empty_room_raw_tsss = mne.preprocessing.maxwell_filter(\n",
    "    empty_room_raw,\n",
    "    st_duration=10,\n",
    "    ignore_ref=True,\n",
    "    st_correlation=0.9,\n",
    "    st_only=True\n",
    ")\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "fname = output_dir + '/' + sub + '_emptyroom_preproc-raw.fif'\n",
    "# mne.io.Raw.save(empty_room_raw_tsss, fname, overwrite=True)"
   ],
   "id": "23fdaa123f5401d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ICA\n",
    "\n",
    "We will now perform ICA on the data. The purpose of ICA is to remove muscular artifacts from the MEG data. These artifacts are eye blinks, heart beats, and saccades. Therefore, we will not run ICA on our empty room data.\n",
    "\n",
    "We will fit our ICA model ot a 1Hz high-pass filtered version of the data to eliminate any slow drifts that would decrease the calculated independence of sources.\n",
    "\n",
    "To carry out ICA, the MNE function [mne.preprocessing.ICA](https://mne.tools/stable/generated/mne.preprocessing.ICA.html) first conducts a principal components analysis (PCA) to whiten the data. The resulting PCA components explain 99% of the variance. Then, it will pass the first 30 components to the ICA algorithm. We will fit the model using the fastica method, which is default."
   ],
   "id": "8789272aa6bd6219"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Below are examples of each type of artifact as seen on the topographic and timecourse plots.\n",
    "![artifacts](Artifacts.jpg \"Artifacts\")\n",
    "\n"
   ],
   "id": "597a489f0386d3c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will start with the comprehension. ICA is the one part of preprocessing that needs to be done slightly differently for the two blocks, because different component numbers will refer to artifacts.",
   "id": "e906c3930eff844a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "comp_raw_tsss_filt = comp_raw_tsss.copy().filter(l_freq=1.0, h_freq=None)\n",
    "ica = ICA(n_components=30, max_iter=\"auto\")\n",
    "ica.fit(comp_raw_tsss_filt)\n",
    "\n",
    "ica.plot_sources(comp_raw_tsss, show_scrollbars=True)\n",
    "ica.plot_components()"
   ],
   "id": "cb6d29cfdd1fbfee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "From the topographic maps and timecourse plots, you will need to manually identify components that contain eye blinks, saccades, or heartbeats. Try to not exclude anything that you can't attribute to one of these muscular artifacts.\n",
    "\n",
    "Based on the outputs, **enter the components to be excluded below**"
   ],
   "id": "fb5458f4e205b3c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ica_excluded = [0,1,4,12]",
   "id": "ab695c35a2589f22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then, apply the ICA model to the raw data",
   "id": "7ab9dbd1123f7950"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ica.exclude = ica_excluded\n",
    "ica.apply(comp_raw_tsss)"
   ],
   "id": "e920535a275ae420",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can now [interpolate](https://mne.tools/stable/generated/mne.io.Raw.html#mne.io.Raw.interpolate_bads) the values for the bad channels, using values from neighboring sensors to infer the activity that *would* have been in that sensor if it were functioning properly",
   "id": "9e493ca8ccece385"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "comp_raw_tsss = comp_raw_tsss.interpolate_bads()",
   "id": "bf167124f3f81635",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we can save the \"cleaned\" continuous file.",
   "id": "5045b218be0a994"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fname = output_dir + '/' + sub + '_comp_preproc-raw.fif'\n",
    "mne.io.Raw.save(comp_raw_tsss, fname, overwrite=True)"
   ],
   "id": "7c5eacb5fab7c0dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will do the same for the production block.",
   "id": "4d7e1264f50b0635"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# High pass filter at 1Hz\n",
    "prod_raw_tsss_filt = prod_raw_tsss.copy().filter(l_freq=1.0, h_freq=None)\n",
    "\n",
    "#Fit ICA model with 30 components\n",
    "ica = ICA(n_components=30, max_iter=\"auto\")\n",
    "ica.fit(prod_raw_tsss_filt)\n",
    "\n",
    "#Plot ICA component scalp topographies and timecourses\n",
    "ica.plot_sources(prod_raw_tsss, show_scrollbars=True)\n",
    "ica.plot_components()"
   ],
   "id": "5680e32a3309f4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Enter the components to exclude**",
   "id": "33456b918887eb90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ica_excluded = [0,1,2,3,9]",
   "id": "7983a86a7cb548e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ica.exclude = ica_excluded\n",
    "ica.apply(prod_raw_tsss)"
   ],
   "id": "70b9dc185822b8c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "prod_raw_tsss = prod_raw_tsss.interpolate_bads()",
   "id": "30d4d4e5f052a763",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fname = output_dir + '/' + sub + '_prod_preproc-raw.fif'\n",
    "mne.io.Raw.save(prod_raw_tsss, fname, overwrite=True)"
   ],
   "id": "1933619b28daf291",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The continuous data should now look much \"cleaner\" that it did originally. Let's take a look at the raw and preprocessed files.",
   "id": "1b197d2d2ca68ea1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "comp_raw.plot()",
   "id": "bbceaef591f7075d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "comp_raw_tsss.plot()",
   "id": "2c703e49c070828",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Create Epoch objects for each block\n",
    "\n",
    "We will now find events that are indicated by trigger channels. We will use separate dictionaries for comprehension and production.\n",
    "\n",
    "The keys in the event dictionary are the names that you would like to refer to the events as, and the values are the trigger channel number. The trigger channel values are set by the researcher in the experimental script.\n",
    "\n",
    "It is best practice to include a text file that keeps track of these values. The text files for this experiment are within the parent folder, and the values are as follows:"
   ],
   "id": "af508275c31f68b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Comprehension**\\\n",
    "*Practice*\\\n",
    "137 - Audio onset\\\n",
    "138 - audio offset\\\n",
    "139 - Word \\\n",
    "? Word identical to prime? \\\n",
    "140 - yes comprehension question \\\n",
    "141 - no comprehension question\n",
    "\n",
    "*Experiment*\\\n",
    "130 - Word Identical to prime (50x)\\\n",
    "132 - Word unrelated to prime (50x)\\\n",
    "136 - Picture unrelated in both lists (50x)\\\n",
    "134 — Audio onset (150x)\\\n",
    "135 — Audio offset (150x)\n",
    "\n",
    "Buttons\\\n",
    "142 - LEFT\\\n",
    "143 - RIGHT\n",
    "\n",
    "Questions:\\\n",
    "Yes - 146\\\n",
    "No - 147\n",
    "\n",
    "**Production**\\\n",
    "*Practice* \\\n",
    "137 - Audio onset \\\n",
    "138 - audio offset\\\n",
    "139 - picture identical to prime \\\n",
    "140 - picture unrelated to prime\n",
    "\n",
    "Experiment \\\n",
    "130 - Picture Identical to prime (50x)\\\n",
    "132 - Word unrelated to prime (50x)\\\n",
    "136 - Picture unrelated in both lists (50x) \\\n",
    "131 - Production trigger (fires after 130/132) \\\n",
    "134 — Audio onset (150x)\\\n",
    "135 — Audio offset (150x)\n",
    "\n",
    "Buttons\\\n",
    "142 - LEFT \\\n",
    "143 - RIGHT"
   ],
   "id": "d64a6161324ec005"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We will now define our events dictionary for the comprehension condition. We are only going to be analyzing the target onsets, so our dictionary will only include those event codes.\n",
    "\n",
    "In the production block, the data are epoched based on the onset of the target image. In the comprehension block, the data are epoched based on the onset of the target word. In other words, when looking at evoked responses, t=0 represents the onset of the image/word.\n",
    "\n",
    "We will keep the ignore condition (words that were never targets in any counterbalancing, or, fillers) for reasons that will be clear later."
   ],
   "id": "7ddf5f67134219b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "event_dict = {\n",
    "    \"comprehension identical\": 162,\n",
    "    \"comprehension unrelated\": 164,\n",
    "    \"ignore\": 168\n",
    "}"
   ],
   "id": "63306b005f999ec0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You may notice that the keys in this dictionary do not match the values from the text file. This is because the value that you input into your script the \"binary\" trigger value, while the values contained in the .sqd file is the \"MEG160\" trigger value. Below is the conversion table between binary and MEG160 (hint, just add 32)\n",
    "\n",
    "![conversions](trigger_conversions.png \"Trigger Conversions\")\n",
    "\n",
    "We will now use our event dictionary to \"label\" trials of interest and then save the data as an [epoch object](https://mne.tools/stable/generated/mne.Epochs.html).\n"
   ],
   "id": "23ea7b5500134695"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Comprehension\n",
    "events = mne.find_events(comp_raw_tsss, stim_channel=\"STI 014\")\n",
    "\n",
    "epochs_comp = mne.Epochs(comp_raw_tsss, events, tmin=-0.3, tmax=0.6, event_id=event_dict, preload=True)"
   ],
   "id": "952f83fa5fc5627e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We also don't want any trials where the maximum peak amplitudes exceed a threshold, which would suggest that a movement artifact, power surge, or some other impurity pervaded in the data through our preprocessing, so we will also excludeepochs that have a max peak to peak signal amplitude that exceeds 3 picoteslas.",
   "id": "5970dbbec0964339"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "reject_criteria = dict(mag=3000e-15)\n",
    "epochs_comp.drop_bad(reject=reject_criteria)"
   ],
   "id": "3168610125310178",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will now save our comprehension epoch object",
   "id": "86e12ba277392a1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fname = output_dir + '/' + sub + '_comp-epo.fif'\n",
    "# epochs_comp.save(fname, overwrite=True)"
   ],
   "id": "e4a0ca2c8eb062f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will do the same thing with the production block.",
   "id": "9367ee79b4280604"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "events = mne.find_events(prod_raw_tsss, stim_channel=\"STI 014\")\n",
    "\n",
    "event_dict = {\n",
    "    \"production identical\": 162,\n",
    "    \"production unrelated\": 164,\n",
    "    \"ignore\": 168\n",
    "}\n",
    "epochs_prod = mne.Epochs(prod_raw_tsss, events, tmin=-0.3, tmax=0.7, event_id=event_dict, preload=True)"
   ],
   "id": "2d7d14d78ccd99b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Instead of saving this epoch object immediately, we will **exclude trials that contain bad productions** (where the participant did not name the object correctly).\n",
    "\n",
    "The production data are stored in the .xlsx file \"Productions_A2\" in the tutorial folder. As a refresher, A2 is the production condition in the \"G\" counterbalancing for the experiment.\n",
    "\n",
    "In this excel sheet, we have information about the intended word, the trigger code, and what the participant produced. With this informaiton, we know if the participant was correct, and if the word had been repetition primed or not."
   ],
   "id": "fb046a7c67ef2a80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "excel_fname = f\"{ROOT}Tutorial/Productions_{prod_condition}.xlsx\"\n",
    "df = pd.read_excel(excel_fname)\n",
    "column_name = sub + ' Accuracy'\n",
    "bad_productions = df.index[df[column_name] == False].tolist()"
   ],
   "id": "9fe35a7b6bf42588",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This is the part where it matters that we kept the \"ignore\" condition. The way that we will exclude the bad productions is by using the index of that trial from the excel sheet. The excel sheet contains all words, those that were repetition primed, unrelated primed, and filler words (150 words). If we did not keep the \"ignore\" condition in our events dictionary, then we would only have 100 items in our epochs object and we would not be able to exclude trials using the indeces from the excel sheet.\n",
    "\n",
    "Let's look at our epochs object containing **all** productions"
   ],
   "id": "3221281e232d5f93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(epochs_prod)\n",
    "len(epochs_prod)"
   ],
   "id": "c8e74d5de3025cea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's drop the bad productions",
   "id": "5576036fb69b06f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "epochs_prod.drop(bad_productions)",
   "id": "512293e75c9f44be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's look at our epoch object again.",
   "id": "9992ce4a2b3f0984"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(epochs_prod)\n",
    "len(epochs_prod)"
   ],
   "id": "ddc95cccdaa3776a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can now see that this particular participant had 30 incorrect productions total. 15 of those bad productions were for filler words and the other 15 were for unrelated primed words. All 50 identically primed words were produced correctly.\n",
    "\n",
    "We will again reject bad epochs that have a max peak to peak signal amplitude that exceeds 3 picoteslas. It's important to do this step **after** excluding the bad productions to ensure that the indexing works properly."
   ],
   "id": "c6c7ca1208474c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "reject_criteria = dict(mag=3000e-15)\n",
    "epochs_prod.drop_bad(reject=reject_criteria)"
   ],
   "id": "c773841dc260dc65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will now save the production epochs.\n",
   "id": "6cd3fcfa164ae420"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fname = output_dir + '/' + sub + '_prod-epo.fif'\n",
    "# epochs_prod.save(fname, overwrite=True)"
   ],
   "id": "7e85448a7c882e18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Equalizing epoch counts",
   "id": "1c941fa335a9894"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We don't want to have uneven counts for each condition (production identical, production unrelated, comprehension identical, comprehension unrelated) when we are doing data analysis, because higher power for some conditions will bias the source estimates.\n",
    "\n",
    " We will equalize epoch counts by creating separate objects for each condition and then randomly excluding trials from objects that have a larger epoch count than the minimum, until they are all the same. In this subject, the lowest epoch count was in the production unrelated condition, so after this function is called, the other three conditions will have the same number of epochs as that condition."
   ],
   "id": "a8946a0b92223920"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ident_prod = epochs_prod['production identical'].pick('mag')\n",
    "unrel_prod = epochs_prod['production unrelated'].pick('mag')\n",
    "ident_comp = epochs_comp['comprehension identical'].pick('mag')\n",
    "unrel_comp = epochs_comp['comprehension unrelated'].pick('mag')\n",
    "mne.epochs.equalize_epoch_counts([ident_prod, ident_comp, unrel_prod, unrel_comp], method=\"random\")"
   ],
   "id": "db41f4dae4a0aba9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create evoked objects for each condition",
   "id": "6d0c2e628caba993"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that the epoch counts are equal, we can make evoked objects for each condition, which contain the average response for each condition. We low pass filter the data at 40 Hz because any frequency higher than that is not relevant to the study.",
   "id": "82d9f9337cdee8d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ident_prod = ident_prod.average().filter(l_freq=None, h_freq=40)\n",
    "unrel_prod = unrel_prod.average().filter(l_freq=None, h_freq=40)\n",
    "ident_comp = ident_comp.average().filter(l_freq=None, h_freq=40)\n",
    "unrel_comp = unrel_comp.average().filter(l_freq=None, h_freq=40)"
   ],
   "id": "bda12e2d56d91c6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Plotting your evoked data\n",
    "\n",
    "We can take a look at our evoked data by plotting the timecourse and topography of different conditions."
   ],
   "id": "cb4a29e680bb8630"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "evokeds_dict = {\n",
    "    \"Prod Ident\": ident_prod,\n",
    "    \"Prod Unrel\": unrel_prod,\n",
    "    \"Comp Ident\": ident_comp,\n",
    "    \"Comp Unrel\": unrel_comp,\n",
    "}"
   ],
   "id": "563b6ee85a16a860",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## All sensors timecourse",
   "id": "6b34175391f641ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Timecourse over all sensors\n",
    "mne.viz.plot_compare_evokeds(evokeds_dict, picks='mag', colors=['red', 'red', 'blue', 'blue'], linestyles=['-', '--', '-', '--'])"
   ],
   "id": "bddc5019aa57436f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Subset of sensors timecourse",
   "id": "c0c37148dd64821"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get subsets of sensors\n",
    "# left posterior\n",
    "left_post_numbers = [4, 5, 6, 7, 8, 9, 34, 36, 37, 38, 40, 47, 48, 49, 50, 75, 76, 77, 79, 88, 127, 129,\n",
    "                     137, 89, 92, 94, 12, 10, 11, 35, 46, 51, 72, 73, 74, 78, 91, 125, 126, 138, 140, 141, 128, 41]\n",
    "left_post = []\n",
    "for i in left_post_numbers:\n",
    "    title = \"\"\n",
    "    if i < 10:\n",
    "        title = 'MEG 00' + str(i)\n",
    "    elif 10 <= i < 100:\n",
    "        title = 'MEG 0' + str(i)\n",
    "    else:\n",
    "        title = 'MEG ' + str(i)\n",
    "    left_post.append(title)\n",
    "\n",
    "#left anterior\n",
    "left_ant_numbers = [1, 2, 3, 39, 42, 43, 44, 80, 81, 86, 83, 84, 85, 108, 130, 131, 132, 133, 134, 135,\n",
    "                    136, 151, 65, 59, 152, 53, 68, 143, 105, 106, 107, 109, 45, 111]\n",
    "left_ant = []\n",
    "for i in left_ant_numbers:\n",
    "    title = \"\"\n",
    "    if i < 10:\n",
    "        title = 'MEG 00' + str(i)\n",
    "    elif 10 <= i < 100:\n",
    "        title = 'MEG 0' + str(i)\n",
    "    else:\n",
    "        title = 'MEG ' + str(i)\n",
    "    left_ant.append(title)\n",
    "print(left_ant)\n",
    "\n",
    "#right posterior\n",
    "right_post_numbers = [14, 15, 16, 17, 18, 19, 27, 28, 30, 54, 56, 57, 66, 69, 70, 97, 119, 121, 122,\n",
    "                      90, 87, 71, 52, 82, 58, 67, 95, 26, 145, 13, 29, 31, 32, 33, 120, 123, 124, 142]\n",
    "right_post = []\n",
    "for i in right_post_numbers:\n",
    "    title = \"\"\n",
    "    if i < 10:\n",
    "        title = 'MEG 00' + str(i)\n",
    "    elif 10 <= i < 100:\n",
    "        title = 'MEG 0' + str(i)\n",
    "    else:\n",
    "        title = 'MEG ' + str(i)\n",
    "    right_post.append(title)\n",
    "\n",
    "#right anterior\n",
    "right_ant_numbers = [20, 21, 22, 23, 24, 60, 61, 63, 99, 100, 114, 115, 116, 117, 118, 147,\n",
    "                     148, 155, 96, 25, 62, 64, 98, 101, 102, 103, 104, 112, 113, 119, 93]\n",
    "right_ant = []\n",
    "for i in right_ant_numbers:\n",
    "    title = \"\"\n",
    "    if i < 10:\n",
    "        title = 'MEG 00' + str(i)\n",
    "    elif 10 <= i < 100:\n",
    "        title = 'MEG 0' + str(i)\n",
    "    else:\n",
    "        title = 'MEG ' + str(i)\n",
    "    right_ant.append(title)"
   ],
   "id": "5d82b521d58a9eec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot subsets of sensors\n",
    "fig, axes = plt.subplots(2, 2, figsize=(13, 10))\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "ylim = dict(mag=[0, 45])\n",
    "# Top-left\n",
    "mne.viz.plot_compare_evokeds(\n",
    "    evokeds_dict,\n",
    "    picks=left_post,\n",
    "    axes=axes[1, 0],\n",
    "    colors=['red', 'red','blue', 'blue'], linestyles=['-', '--', '-', '--'],\n",
    "    show=False,\n",
    "    ylim=ylim\n",
    ")\n",
    "axes[1, 0].set_title(\"Left Posterior\",size=20)\n",
    "\n",
    "# Top-right\n",
    "mne.viz.plot_compare_evokeds(\n",
    "    evokeds_dict,\n",
    "    picks=left_ant,\n",
    "    axes=axes[0, 0],\n",
    "    colors=['red', 'red','blue', 'blue'], linestyles=['-', '--', '-', '--'],\n",
    "    show=False,\n",
    "    ylim=ylim\n",
    ")\n",
    "axes[0, 0].set_title(\"Left Anterior\",size=20)\n",
    "\n",
    "# Bottom-left\n",
    "mne.viz.plot_compare_evokeds(\n",
    "    evokeds_dict,\n",
    "    picks=right_post,\n",
    "    axes=axes[1, 1],\n",
    "    colors=['red', 'red','blue', 'blue'], linestyles=['-', '--', '-', '--'],\n",
    "    show=False,\n",
    "    ylim=ylim\n",
    ")\n",
    "axes[1, 1].set_title(\"Right Posterior\", size=20)\n",
    "\n",
    "# Bottom-right\n",
    "mne.viz.plot_compare_evokeds(\n",
    "    evokeds_dict,\n",
    "    picks=right_ant,\n",
    "    axes=axes[0, 1],\n",
    "    colors=['red', 'red','blue', 'blue'], linestyles=['-', '--', '-', '--'],\n",
    "    show=False,\n",
    "    ylim=ylim\n",
    ")\n",
    "axes[0, 1].set_title(\"Right Anterior\",size=20)\n",
    "\n",
    "plt.show()"
   ],
   "id": "145a15c157f260a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Topographic difference plots",
   "id": "f45efe4fc0418e77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Difference between unrelated and identical primed conditions for each task\n",
    "comp_dif = mne.combine_evoked([unrel_comp, ident_comp], weights = [1, -1])\n",
    "prod_dif = mne.combine_evoked([unrel_prod, ident_prod], weights = [1,-1])\n",
    "\n",
    "times = np.arange(0.35, 0.5, 0.025)\n",
    "fig = comp_dif.plot_topomap(times, ch_type=\"mag\", show=False, vlim=(-45, 45))\n",
    "plt.suptitle('Comprehension Unprimed-Primed', fontsize=20, color='blue')\n",
    "plt.show()\n",
    "\n",
    "times = np.arange(0.35, 0.5, 0.025)\n",
    "fig = prod_dif.plot_topomap(times, ch_type=\"mag\", show=False, vlim=(-45,45))\n",
    "plt.suptitle('Production Unprimed-Primed', fontsize=20, color='red')\n",
    "plt.show()"
   ],
   "id": "8cc9da72d06011d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Obtain STCs (Source Localization!)",
   "id": "81b940b7f8ddd7f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Inferring source space activity from  sensor data requires that you have **FreeSurfer** installed, and that you carried out coregistration. Coregistration will produce an anatomical model of the subject's brain based on the digitization as well as yielding a transformation matrix.\n",
    "\n",
    "To install FreeSurfer, follow the instructions on [this page](https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall).\n",
    "\n",
    "In this tutorial, coregistration has been done for you. All the results can be found in the Tutorial/freesurfer/R3289 folder. You can follow the instructions in [this document](https://docs.google.com/document/d/16D3C3NQU8hzEc976RHI2Z3RJRwI9mlbs-yQkQ6N0To8/edit?usp=sharing) to do it yourself."
   ],
   "id": "6530827d09f3f7b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "subjects_dir = f\"{ROOT}Tutorial/freesurfer/\"\n",
    "directory = f\"{ROOT}Tutorial/{sub}/\""
   ],
   "id": "678b21bb2ec71384",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will set some generic parameters for source localization. More information can be found here.",
   "id": "7e300fd6665cce4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "conductivity = (0.3,) # single layer conductivity\n",
    "baseline_start = -300 #in milliseconds\n",
    "baseline_end = 0\n",
    "baseline = (None,0)\n",
    "snr = 3.0\n",
    "method = \"dSPM\""
   ],
   "id": "2100be5058e5fa71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we will create the anatomical model of our subject. BEM model documentation can be found [here](https://mne.tools/stable/generated/mne.make_bem_model.html), and source space documentation can be found [here](https://mne.tools/stable/generated/mne.setup_source_space.html).",
   "id": "ab3a2b40978015b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "subject = sub\n",
    "model = mne.make_bem_model(subject=subject, ico=4, conductivity=conductivity, subjects_dir=subjects_dir)\n",
    "bem = mne.make_bem_solution(model)\n",
    "src = mne.setup_source_space(subject, spacing=\"oct6\", add_dist=\"patch\", subjects_dir=subjects_dir)"
   ],
   "id": "c2397da9cb4fe28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We will now compute the noise covariance using the baseline interval, which spans from the beginning of the evoked timescale (in this case, -300ms), to the image/word onset (0ms).\n",
    "\n",
    "It is common practice to use the [baseline interval for noise covariance](https://mne.tools/stable/auto_tutorials/forward/90_compute_covariance.html) in evoked experiments, though one could also use the empty room file.\n",
    "\n",
    "The MNE function does not automatically calculate the noise covariance rank accurately, in my experience. This could be related to the fact that the automatic rank computation does not do a good job taking IC removal during ICA into account. [Here](https://github.com/mne-tools/mne-python/issues/7727) and [here](https://mne.discourse.group/t/specify-noise-covariance-rank-in-mne-python/888/5) are threads that discusses the issue.\n",
    "\n",
    "My approach is to set the rank manually. Basically, you want to find a numerical value that sets the red dotted line right at the edge of the \"shelf\" of the curve of noise plotted over eigenvalues.\n",
    "**Below, change X in {'mag': X} to a value that sets the vertical dotted line at the correct position on the curve.**\n",
    "\n",
    "Let me know if you find a better, automatic way to do this step."
   ],
   "id": "d36d39c92dbdb2da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rank = {'mag': 149}\n",
    "noise_cov_comp = mne.compute_covariance(epochs_comp, tmin=baseline_start, tmax=baseline_end,\n",
    "                                        method=[\"shrunk\"], verbose=True)\n",
    "mne.viz.plot_cov(noise_cov_comp, epochs_comp.info)"
   ],
   "id": "be2cc6c64b6c5ce4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once the rank value looks good, you can compute the forward solution.",
   "id": "81c73df5fc79ed83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trans = input_dir + sub + '-trans.fif'\n",
    "fname = output_dir + '/' + sub + '_comp_preproc-raw.fif'\n",
    "\n",
    "# Compute forward solution\n",
    "fwd_comp = mne.make_forward_solution(\n",
    "    fname,\n",
    "    trans=trans,\n",
    "    src=src,\n",
    "    bem=bem,\n",
    "    meg=True,\n",
    "    eeg=False,\n",
    "    mindist=5.0,\n",
    "    ignore_ref=True\n",
    ")"
   ],
   "id": "f6ca1b02e397aec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then, you can compute the inverse solution",
   "id": "8de13cc39f77f77b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lambda2 = 1.0 / snr ** 2\n",
    "\n",
    "# Calculate inverse operators\n",
    "inverse_ident_comp = make_inverse_operator(ident_comp.info, fwd_comp, noise_cov_comp, loose=0.2, depth=0.8)\n",
    "inverse_unrel_comp = make_inverse_operator(unrel_comp.info, fwd_comp, noise_cov_comp, loose=0.2, depth=0.8)"
   ],
   "id": "b44a5c18d77fdb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then, you can imply the inverse operator to obtain and save your source time courses.",
   "id": "33f7290904dce604"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate STCs using evoked data, inverse operators from above\n",
    "stc_ident_comp = apply_inverse(ident_comp, inverse_ident_comp, lambda2, method=method, pick_ori=\"normal\")\n",
    "stc_unrel_comp = apply_inverse(unrel_comp, inverse_unrel_comp, lambda2, method=method, pick_ori=\"normal\")"
   ],
   "id": "89a23070001d50fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A good inverse solution will explain a large amount of the variance in the original data. If your solution explains <65% of the variance, you should try to find a better value for rank, or try to identify if anything could be contributing to a low SNR.\n",
    "\n",
    "If the inverse solution looks good, you can save your files."
   ],
   "id": "81c3e5c95b68e681"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fname_ident_comp = output_dir + sub + '_ident_comp'\n",
    "stc_ident_comp.save(fname_ident_comp, ftype='stc', overwrite=True)\n",
    "fname_unrel_comp = output_dir + sub + '_unrel_comp'\n",
    "stc_unrel_comp.save(fname_unrel_comp, ftype='stc', overwrite=True)"
   ],
   "id": "bcf84e77eb52df8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Do the exact same for production.",
   "id": "ebffbc3c6e6f69c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rank = {'mag': 148}\n",
    "noise_cov_prod = mne.compute_covariance(epochs_prod, tmin=baseline_start, tmax=baseline_end, method=[\"shrunk\", \"empirical\"], rank=rank, verbose=True)\n",
    "mne.viz.plot_cov(noise_cov_prod, epochs_prod.info)"
   ],
   "id": "746476da119d4fd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fname = output_dir + '/' + sub + '_prod_preproc-raw.fif'\n",
    "trans = input_dir + sub + '-trans.fif'\n",
    "\n",
    "# Compute forward solution\n",
    "fwd_prod = mne.make_forward_solution(\n",
    "    fname,\n",
    "    trans=trans,\n",
    "    src=src,\n",
    "    bem=bem,\n",
    "    meg=True,\n",
    "    eeg=False,\n",
    "    mindist=5.0,\n",
    "    ignore_ref=True\n",
    ")\n",
    "lambda2 = 1.0 / snr ** 2\n",
    "\n",
    "# Calculate inverse operators\n",
    "inverse_ident_prod = make_inverse_operator(ident_prod.info, fwd_prod, noise_cov_prod, loose=0.2, depth=0.8)\n",
    "inverse_unrel_prod = make_inverse_operator(unrel_prod.info, fwd_prod, noise_cov_prod, loose=0.2, depth=0.8)\n",
    "\n",
    "# Calculate STCs using evoked data, inverse operators from above\n",
    "stc_ident_prod = apply_inverse(ident_prod, inverse_ident_prod, lambda2, method=method, pick_ori=\"normal\")\n",
    "stc_unrel_prod = apply_inverse(unrel_prod, inverse_unrel_prod, lambda2, method=method, pick_ori=\"normal\")\n",
    "\n",
    "fname_ident_comp = directory + sub + '_ident_prod'\n",
    "stc_ident_prod.save(fname_ident_comp, ftype='stc', overwrite=True)\n",
    "fname_unrel_comp = directory + sub + '_unrel_prod'\n",
    "stc_unrel_prod.save(fname_unrel_comp, ftype='stc', overwrite=True)"
   ],
   "id": "55ef64367be2769f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Plot STCs",
   "id": "6932b4d4c012cc3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Individual Conditions",
   "id": "c98775b7abb1e482"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "brain = stc_ident_prod.plot(\n",
    "    subjects_dir=subjects_dir,\n",
    "    hemi='lh',\n",
    "    clim=dict(kind=\"value\", lims=[0,2,4]),\n",
    "    smoothing_steps=7,\n",
    "    initial_time=0.4,\n",
    "    background='white',\n",
    "    show_traces=False,\n",
    "    title='Production Repetition Primed')"
   ],
   "id": "f1c5bd1f48e5d09d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "brain = stc_unrel_prod.plot(\n",
    "    subjects_dir=subjects_dir,\n",
    "    hemi='lh',\n",
    "    clim=dict(kind=\"value\", lims=[0,2,4]),\n",
    "    smoothing_steps=7,\n",
    "    initial_time=0.4,\n",
    "    background='white',\n",
    "    show_traces=False,\n",
    "    title='Production Unrelated Primed')"
   ],
   "id": "bd1ca5f5478a00f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "brain = stc_ident_comp.plot(\n",
    "    subjects_dir=subjects_dir,\n",
    "    hemi='lh',\n",
    "    clim=dict(kind=\"value\", lims=[0,2,4]),\n",
    "    smoothing_steps=7,\n",
    "    initial_time=0.4,\n",
    "    background='white',\n",
    "    show_traces=False,\n",
    "    title='Comprehension Repetition Primed')"
   ],
   "id": "6db16aa81c3ecc2a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "brain = stc_unrel_comp.plot(\n",
    "    subjects_dir=subjects_dir,\n",
    "    hemi='lh',\n",
    "    clim=dict(kind=\"value\", lims=[0,2,4]),\n",
    "    smoothing_steps=7,\n",
    "    initial_time=0.4,\n",
    "    background='white',\n",
    "    show_traces=False,\n",
    "    title='Comprehension Unrelated Primed')"
   ],
   "id": "3252afe6bf43456b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Differences between conditions",
   "id": "2c3a7994b63b5b80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "stc_dif_prod = stc_unrel_prod - stc_ident_prod\n",
    "stc_dif_comp = stc_unrel_comp - stc_ident_comp"
   ],
   "id": "8d467a754ffa8aa2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "brain = stc_dif_prod.plot(\n",
    "    subjects_dir=subjects_dir,\n",
    "    hemi='lh',\n",
    "    clim=dict(kind=\"value\", lims=[0,2,4]),\n",
    "    smoothing_steps=7,\n",
    "    initial_time=0.4,\n",
    "    background='white',\n",
    "    show_traces=False,\n",
    "    title='Production Difference')"
   ],
   "id": "c1c42a35f60b8e3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "brain = stc_dif_comp.plot(\n",
    "    subjects_dir=subjects_dir,\n",
    "    hemi='lh',\n",
    "    clim=dict(kind=\"value\", lims=[0,2,4]),\n",
    "    smoothing_steps=7,\n",
    "    initial_time=0.4,\n",
    "    background='white',\n",
    "    show_traces=False,\n",
    "    title='Comprehension Difference')"
   ],
   "id": "6cf118aafc4c813a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
